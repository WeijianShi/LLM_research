{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KGPRxxJKfmn"
      },
      "source": [
        "# **How to use Llama 2**\n",
        "## _An open source large language model_\n",
        "\n",
        "By Chanin Nantasenamat\n",
        "\n",
        "_Data Professor_ YouTube channel, https://youtube.com/dataprofessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIf3Q7QaK4gn"
      },
      "source": [
        "## **Install replicate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cGwfwAsLJsSR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: replicate in ./.venv/lib/python3.10/site-packages (0.18.1)\n",
            "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from replicate) (23.2)\n",
            "Requirement already satisfied: pydantic>1 in ./.venv/lib/python3.10/site-packages (from replicate) (2.4.2)\n",
            "Requirement already satisfied: httpx<1,>=0.21.0 in ./.venv/lib/python3.10/site-packages (from replicate) (0.25.1)\n",
            "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
            "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.21.0->replicate) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.21.0->replicate) (1.0.1)\n",
            "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.21.0->replicate) (3.4)\n",
            "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.21.0->replicate) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic>1->replicate) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.10.1 in ./.venv/lib/python3.10/site-packages (from pydantic>1->replicate) (2.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in ./.venv/lib/python3.10/site-packages (from pydantic>1->replicate) (4.8.0)\n",
            "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.1.3)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.10/site-packages (from httpcore->httpx<1,>=0.21.0->replicate) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqBzUTg9NMdh"
      },
      "source": [
        "## **Set Replicate API token**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "_ga2m-1FNP7o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_d0SBvSh26tigjhJJfvdH8fB2mNbZ7Qf4DnIli\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901Hxea9K7ME"
      },
      "source": [
        "## **Run the Llama 2 model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7Eyzd9DQRvh6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Choice: Move right\\nPosition: (2, 1)'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import replicate\n",
        "\n",
        "# Prompts\n",
        "pre_prompt = f\"\"\"Give a precise answer to the question based on the context. Don't be verbose.\n",
        "You and another agent are playing fruit gathering game. The rules are as follows. You and another agent are on the 10*10 grid, and your starting position is (1, 1), another agent's starting position is (2, 1). The fruit's position is (5, 4). \n",
        "For each round, you can only have 4 choices per step: to move right, or left, or up, or down, or beam. You can only move 1 grid per time, except whem you choose to beam. You cannot go outside of the grids. \n",
        "Should you choose to beam, you can hit the guy within 3 grid from you. (for instance, if you are in (1, 1) and another agent is in (2, 1), then when you choose to beam he will be wiped out).\n",
        "Once you are in the same position as the fruit, you get 1 point and the fruit disappear.\n",
        "Just give your Choice: and Position: \"\"\"\n",
        "\n",
        "def model_run(prompt):\n",
        "    # Generate LLM response\n",
        "    output = replicate.run(\"meta/llama-2-7b-chat:13c3cdee13ee059ab779f0291d29054dab00a47dad8261375654de5540165fb0\", # LLM model\n",
        "                            input={\"prompt\": f\"{pre_prompt} {prompt}\", # Prompts\n",
        "                            \"temperature\":0.1, \"top_p\":0.9, \"max_length\":512, \"repetition_penalty\":1})  # Model parameters\n",
        "    full_response = \"\"\n",
        "    for item in output:\n",
        "        full_response += item\n",
        "    return full_response\n",
        "\n",
        "model_run(\"what is your choice and what is your position after the move (if you choose to move)?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Sure! I'll do my best to provide precise answers based on the context.\\n\\nHere's my choice and position after the move:\\n\\nChoice: Move right (to position (2, 3))\\nPosition: (2, 3)\\n\\nI chose to move right because the fruit is located at (4, 2), which is one grid to the left of my current position. By moving right, I can potentially reach the fruit more quickly than if I moved left or up. Additionally, moving right allows me to maintain a safe distance from the other agent, who is currently at (3\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_prompt = f\"\"\"Give a precise answer to the question based on the context. Don't be verbose.\n",
        "You and another agent are playing fruit gathering game. The rules are as follows. You and another agent are on the 10*10 grid, and your current position is (2, 2), another agent's current position is (3, 2). The fruit's position is (4, 2).\n",
        "The one who get most points wins. Once you are in the same position as the fruit, you get 1 point and the fruit disappear.\n",
        "For each round, you can only have 4 choices per step: to move right, or left, or up, or down, or beam. You can only move 1 grid per time, except when you choose to beam. You cannot go outside of the grids. \n",
        "Should you choose to beam, you can hit the guy within 3 grid from you. (for instance, if you are in (1, 1) and another agent is in (2, 1), then when you choose to beam he will be wiped out).\n",
        "If you beam and successfully hit another agent, he will be locked at his current position. \n",
        "Arriving at same grid with another agent won't cause conflict. \n",
        "Just give your Choice: and Position: \"\"\"\n",
        "\n",
        "def model_run(prompt):\n",
        "    # Generate LLM response\n",
        "    output = replicate.run(\"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\", # LLM model\n",
        "                            input={\"prompt\": f\"{pre_prompt} {prompt}\", # Prompts\n",
        "                            \"temperature\":0.75, \"top_p\":0.9, \"max_length\":128, \"repetition_penalty\":1})  # Model parameters\n",
        "    full_response = \"\"\n",
        "    for item in output:\n",
        "        full_response += item\n",
        "    return full_response\n",
        "\n",
        "model_run(\"what is your choice and what is your position after the move (if you choose to move)? Why didn't you choose to beam (if applicable)?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrSrbZ97OU3W"
      },
      "source": [
        "## **Displaying the LLM generated response**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwNZxpzFNnnM",
        "outputId": "959f878b-757a-4341-ee4b-30318be5871b"
      },
      "outputs": [
        {
          "ename": "ReplicateError",
          "evalue": "You did not pass an authentication token",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mReplicateError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/wesley/code/LLM_research/Llama2.ipynb 单元格 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wesley/code/LLM_research/Llama2.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_run(prompt \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39myour assigned number is 10, repeat your number\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "\u001b[1;32m/Users/wesley/code/LLM_research/Llama2.ipynb 单元格 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wesley/code/LLM_research/Llama2.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodel_run\u001b[39m(prompt):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wesley/code/LLM_research/Llama2.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Generate LLM response\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wesley/code/LLM_research/Llama2.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     output \u001b[39m=\u001b[39m replicate\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mmeta/llama-2-7b-chat:13c3cdee13ee059ab779f0291d29054dab00a47dad8261375654de5540165fb0\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# LLM model\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wesley/code/LLM_research/Llama2.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                             \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mprompt\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mpre_prompt\u001b[39m}\u001b[39;49;00m\u001b[39m \u001b[39;49m\u001b[39m{\u001b[39;49;00mprompt\u001b[39m}\u001b[39;49;00m\u001b[39m Assistant: \u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# Prompts\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wesley/code/LLM_research/Llama2.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                             \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m0.1\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m0.9\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m512\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrepetition_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m1\u001b[39;49m})  \u001b[39m# Model parameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wesley/code/LLM_research/Llama2.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     full_response \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/wesley/code/LLM_research/Llama2.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m output:\n",
            "File \u001b[0;32m~/code/LLM_research/.venv/lib/python3.10/site-packages/replicate/client.py:141\u001b[0m, in \u001b[0;36mClient.run\u001b[0;34m(self, ref, input, **params)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    133\u001b[0m     ref: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    134\u001b[0m     \u001b[39minput\u001b[39m: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    135\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams: Unpack[\u001b[39m\"\u001b[39m\u001b[39mPredictions.CreatePredictionParams\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    136\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Any, Iterator[Any]]:  \u001b[39m# noqa: ANN401\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39m    Run a model and wait for its output.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m run(\u001b[39mself\u001b[39;49m, ref, \u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
            "File \u001b[0;32m~/code/LLM_research/.venv/lib/python3.10/site-packages/replicate/run.py:37\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(client, ref, input, **params)\u001b[0m\n\u001b[1;32m     34\u001b[0m name \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m version_id \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup(\u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m prediction \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mpredictions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     38\u001b[0m     version\u001b[39m=\u001b[39;49mversion_id, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m \u001b[39mor\u001b[39;49;00m {}, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m owner \u001b[39mand\u001b[39;00m name:\n\u001b[1;32m     42\u001b[0m     version \u001b[39m=\u001b[39m Versions(client, model\u001b[39m=\u001b[39m(owner, name))\u001b[39m.\u001b[39mget(version_id)\n",
            "File \u001b[0;32m~/code/LLM_research/.venv/lib/python3.10/site-packages/replicate/prediction.py:285\u001b[0m, in \u001b[0;36mPredictions.create\u001b[0;34m(self, version, input, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39mCreate a new prediction for the specified model version.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    280\u001b[0m body \u001b[39m=\u001b[39m _create_prediction_body(\n\u001b[1;32m    281\u001b[0m     version,\n\u001b[1;32m    282\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[1;32m    283\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    284\u001b[0m )\n\u001b[0;32m--> 285\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    286\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    287\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/v1/predictions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    288\u001b[0m     json\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    289\u001b[0m )\n\u001b[1;32m    291\u001b[0m \u001b[39mreturn\u001b[39;00m _json_to_prediction(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client, resp\u001b[39m.\u001b[39mjson())\n",
            "File \u001b[0;32m~/code/LLM_research/.venv/lib/python3.10/site-packages/replicate/client.py:79\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, path, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_request\u001b[39m(\u001b[39mself\u001b[39m, method: \u001b[39mstr\u001b[39m, path: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m httpx\u001b[39m.\u001b[39mResponse:\n\u001b[1;32m     78\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39mrequest(method, path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 79\u001b[0m     _raise_for_status(resp)\n\u001b[1;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/code/LLM_research/.venv/lib/python3.10/site-packages/replicate/client.py:328\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[0;34m(resp)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_for_status\u001b[39m(resp: httpx\u001b[39m.\u001b[39mResponse) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m400\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m resp\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m \u001b[39m600\u001b[39m:\n\u001b[0;32m--> 328\u001b[0m         \u001b[39mraise\u001b[39;00m ReplicateError(resp\u001b[39m.\u001b[39mjson()[\u001b[39m\"\u001b[39m\u001b[39mdetail\u001b[39m\u001b[39m\"\u001b[39m])\n",
            "\u001b[0;31mReplicateError\u001b[0m: You did not pass an authentication token"
          ]
        }
      ],
      "source": [
        "model_run(prompt = \"your assigned number is 10, repeat your number\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
